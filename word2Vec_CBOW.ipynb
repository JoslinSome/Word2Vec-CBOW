{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fDEQjuu_iJ3g"
      },
      "source": [
        "# Create a word2vec model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bRrPQL2diJ3j"
      },
      "source": [
        "For this assignment, we will use Pytorch to create a word2vec model that infers numerical vectors for words that capture their meaning. Word2vec was first introduced in 2013 by Mikolov et al. at Google. Their paper can be found [here](https://arxiv.org/pdf/1301.3781.pdf), though you do not need to read and understand it in order to implement the model. It is a very popular machine learning model that has been implemented to capture the meaning of text for many real world cases. \n",
        "\n",
        "This [blog post](https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/) is a great overview of word2vec. Please read it carefully before you create the word2vec model for this assignment. Specifically, you will build a \"Continuous Bag-of-Words Model (CBOW)\" word2vec model. CBOW predicts a focal (target) word from its context (the words surround it). The following Youtube videos also explain the concept of the CBOW model.\n",
        "- https://www.youtube.com/watch?v=UqRCEmrv1gQ\n",
        "- https://www.youtube.com/watch?v=gQddtTdmG_8 \n",
        "\n",
        "\n",
        "<img src=\"https://analyticsindiamag.com/wp-content/uploads/2020/09/q.png\">\n",
        "\n",
        "[CBOW structure from https://analyticsindiamag.com/the-continuous-bag-of-words-cbow-model-in-nlp-hands-on-implementation-with-codes/]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rayw7XfBiJ3u"
      },
      "source": [
        "Your task is to create a CBOW neural network model class called `CBOW`. `CBOW` has the structure shown above and the following properties:\n",
        "\n",
        "- `vocab_size` - Size of vocabulary($V$). Note that vocabulary is a set of unique words in a corpus (a bunch of text documents).\n",
        "- `embed_dim` - Dimension of the embedding vector\n",
        "- `window_size` - Size of window. If a focal word is at position $t$, then the CBOW model uses embedding vectors of words between ($t$-window_size) and ($t$+window_size) to predict the focal word\n",
        "- `hidden_dim` - Dimension of the hidden layer ($N$)\n",
        "\n",
        "`CBOW` consists of three layers:\n",
        "\n",
        "- `embedding` - An embedding layer that is initialized with `torch.nn.Embedding`\n",
        "- `fc1` - A linear transformation that connects the embedding layer to the hidden layer. `torch.nn.functional.relu` activation should be applied to the output of `fc1`.\n",
        "- `fc2` - A linear transformation that connects the activation of `fc1` to a tensor of length `vocab_size`. \n",
        "\n",
        "The training data (i.e., the features `X`, the labels `y`) that we will use to train the `CBOW` model will be:\n",
        "- `X` will be a tensor of length (2 * `window_size`) containing the indices of all words in the window before and after the focal word. \n",
        "- `y`, (the label that our model is trying to predict) should be a list containing the index of the focal word.\n",
        "\n",
        "Note that a single review in our data will produce multiple items of training data. For example, suppose a single review is:\n",
        "\n",
        " \"the food was not good at all\"\n",
        " \n",
        " If our `window_size` = 2, then this would generate the following (context, focal_word) training data tuples:\n",
        "\n",
        "```python\n",
        "(['the','food','not','good'], ['was']) # 'was' is the focal word\n",
        "(['food','was','good','at'], ['not']) # 'not' is the focal word\n",
        "(['was','not','at','all'],['good']) # 'good' is the focal word\n",
        "```\n",
        "\n",
        "However, we can't directly use these tuples to train our model. First we have to replace each word with a unique integer (its index) and then convert these to pytorch tensors. Note that, we will be using a special embedding layer (`torch.nn.Embedding`) which will convert these indexes to the one-hot vectors that are described in the videos.\n",
        "\n",
        "To get tensors from the original data, you will need to:\n",
        "\n",
        "- Create a list (or `set`) of all unique words in the cleaned text, called `vocab`.\n",
        "- Create a dictionary called `word_to_index` where the key is a word and the value is the index of a word (a unique number for each word). You will have to figure out how to create this dictionary from the cleaned dataset.\n",
        "- Write a function `make_cbow_data` that accepts a single review from cleaned_text as an input and outputs a **list of tuples** where:\n",
        " -  the first part of the tuple contains a tensor of the indices of words in the window before and after each focal word\n",
        " - the second part of the tuple is a tensor containing the index of the focal word.\n",
        " - The dtype of both tensors in the tuple should be `torch.long`.\n",
        " - You will have to figure out how to create multiple tuples of tensors from a single review (an item from `cleaned_text`) using loops \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P6uzFDXFiJ3v"
      },
      "source": [
        "We will use restaurant customer reviews data for this assignment.\n",
        "\n",
        "**Do not change the code block below**. Below is a function that cleans up the text of a review and returns a list of all the words in the review.\n",
        "\n",
        "You will use `cleaned_text`, which is defined below, to create a training dataset for your `CBOW` model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "SYTmVZStiJ3w"
      },
      "outputs": [],
      "source": [
        "# DO NOT CHANGE THIS CODEBLOCK\n",
        "import pandas as pd\n",
        "import string\n",
        "import Word2VecSupport\n",
        "\n",
        "def clean_text(text):    \n",
        "    x = text.translate(str.maketrans('', '', string.punctuation)) # remove punctuation\n",
        "    x = x.lower().split() # lower case and split by whitespace to differentiate words\n",
        "    return x\n",
        "\n",
        "example_text = pd.read_csv('text.csv')\n",
        "cleaned_text = example_text.Review[:100].apply(clean_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Description\n",
        "Here, the `example_text` has 1000 pieces of text message. The first 5 pieces of message are shown in the following.\n",
        "\n",
        "Since there exist punctuation in `example_text`, we use `cleaned_text` function to remove them and also set all words to lower case and split them ny whitespace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Review    1000\n",
            "dtype: int64\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wow... Loved this place.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Crust is not good.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Not tasty and the texture was just nasty.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Stopped by during the late May bank holiday of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The selection on the menu was great and so wer...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Review\n",
              "0                           Wow... Loved this place.\n",
              "1                                 Crust is not good.\n",
              "2          Not tasty and the texture was just nasty.\n",
              "3  Stopped by during the late May bank holiday of...\n",
              "4  The selection on the menu was great and so wer..."
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(example_text.count())\n",
        "example_text.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0                            [wow, loved, this, place]\n",
              "1                               [crust, is, not, good]\n",
              "2    [not, tasty, and, the, texture, was, just, nasty]\n",
              "3    [stopped, by, during, the, late, may, bank, ho...\n",
              "4    [the, selection, on, the, menu, was, great, an...\n",
              "Name: Review, dtype: object"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cleaned_text.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HEl72ZkpTaQ-"
      },
      "source": [
        "### Create a CBOW Class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WpUT3gC6P3Dh"
      },
      "source": [
        "The first step is to create `vocab` and `word_to_index` according to the instructions above.\n",
        "\n",
        "Here, to create unique vocabulary, each word in `cleaned_text` was checked through nested for loops and the `set()` function was used to delete redundant words.\n",
        "\n",
        "The resulting `vocab` has a size of 483, which means there is in total 483 unique words in `cleaned_text`. \n",
        "\n",
        "`word_to_index` was a dictionary created to give all 483 words an index number.\n",
        "\n",
        "You can click the variables explorer to find more details about `word_to_index` and `vocab`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "4QQycg3vFhf8"
      },
      "outputs": [],
      "source": [
        "#Create your vocab here\n",
        "intermid = []\n",
        "for i in range(len(cleaned_text)):\n",
        "    for j in range(len(cleaned_text[i])):\n",
        "        intermid.append(cleaned_text[i][j])\n",
        "vocab = set(intermid)\n",
        "\n",
        "#Create your word_to_index dictionary here\n",
        "vocab = list(vocab)\n",
        "\n",
        "# Create word2Vec from the supporting class\n",
        "word2Vec = Word2VecSupport.Word2Vec(vocab)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['wow', 'loved', 'this', 'place']"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cleaned_text[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(tensor([109,  37, 115, 332]), tensor([318])),\n",
              " (tensor([ 37, 318, 332,  23]), tensor([115])),\n",
              " (tensor([318, 115,  23,  72]), tensor([332]))]"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word2Vec.make_cbow_data(['the', 'food', 'was', 'not', 'good', 'at', 'all'], 2, word2Vec.word_to_index_dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'lady': 0,\n",
              " 'waiter': 1,\n",
              " 'jeff': 2,\n",
              " 'fear': 3,\n",
              " 'only': 4,\n",
              " 'just': 5,\n",
              " 'under': 6,\n",
              " 'thing': 7,\n",
              " 'seafood': 8,\n",
              " 'to': 9,\n",
              " 'indicate': 10,\n",
              " 'portions': 11,\n",
              " 'cute': 12,\n",
              " 'finish': 13,\n",
              " 'rice': 14,\n",
              " 'people': 15,\n",
              " 'got': 16,\n",
              " 'as': 17,\n",
              " 'happier': 18,\n",
              " 'sure': 19,\n",
              " 'meat': 20,\n",
              " 'second': 21,\n",
              " 'cafe': 22,\n",
              " 'at': 23,\n",
              " 'dessert': 24,\n",
              " 'sauce': 25,\n",
              " 'delight': 26,\n",
              " 'selection': 27,\n",
              " 'our': 28,\n",
              " 'there': 29,\n",
              " 'final': 30,\n",
              " 'palate': 31,\n",
              " 'rib': 32,\n",
              " 'my': 33,\n",
              " 'dish': 34,\n",
              " 'chocolate': 35,\n",
              " 'and': 36,\n",
              " 'food': 37,\n",
              " 'tried': 38,\n",
              " 'wayyy': 39,\n",
              " 'greek': 40,\n",
              " 'melted': 41,\n",
              " 'cakes': 42,\n",
              " 'cooked': 43,\n",
              " 'well': 44,\n",
              " 'totally': 45,\n",
              " 'hour': 46,\n",
              " 'drag': 47,\n",
              " 'first': 48,\n",
              " 'had': 49,\n",
              " 'dressing': 50,\n",
              " 'pita': 51,\n",
              " 'time': 52,\n",
              " 'coming': 53,\n",
              " 'tables': 54,\n",
              " 'if': 55,\n",
              " 'arrived': 56,\n",
              " 'rightthe': 57,\n",
              " 'generic': 58,\n",
              " 'give': 59,\n",
              " 'grossed': 60,\n",
              " 'overpriced': 61,\n",
              " 'cod': 62,\n",
              " 'bye': 63,\n",
              " 'prime': 64,\n",
              " 'judge': 65,\n",
              " 'also': 66,\n",
              " 'pasta': 67,\n",
              " 'crust': 68,\n",
              " 'gringos': 69,\n",
              " 'apparently': 70,\n",
              " 'cheated': 71,\n",
              " 'all': 72,\n",
              " 'pan': 73,\n",
              " 'frozen': 74,\n",
              " 'his': 75,\n",
              " 'get': 76,\n",
              " 'red': 77,\n",
              " 'signs': 78,\n",
              " 'overall': 79,\n",
              " 'alone': 80,\n",
              " 'loved': 81,\n",
              " 'name': 82,\n",
              " 'parties': 83,\n",
              " 'back': 84,\n",
              " 'flirting': 85,\n",
              " 'right': 86,\n",
              " 'youre': 87,\n",
              " 'found': 88,\n",
              " 'power': 89,\n",
              " 'visit': 90,\n",
              " 'buffet': 91,\n",
              " 'rick': 92,\n",
              " 'wed': 93,\n",
              " 'cranberrymmmm': 94,\n",
              " 'go': 95,\n",
              " 'cashier': 96,\n",
              " 'performed': 97,\n",
              " 'getting': 98,\n",
              " 'disaster': 99,\n",
              " 'chow': 100,\n",
              " 'interior': 101,\n",
              " 'customers': 102,\n",
              " 'mein': 103,\n",
              " 'about': 104,\n",
              " 'grill': 105,\n",
              " 'included': 106,\n",
              " 'an': 107,\n",
              " '23': 108,\n",
              " 'the': 109,\n",
              " 'tell': 110,\n",
              " 'huge': 111,\n",
              " 'much': 112,\n",
              " 'grab': 113,\n",
              " 'elsewhere': 114,\n",
              " 'not': 115,\n",
              " 'still': 116,\n",
              " 'their': 117,\n",
              " 'sides': 118,\n",
              " 'went': 119,\n",
              " 'lunch': 120,\n",
              " 'tastings': 121,\n",
              " 'we': 122,\n",
              " 'want': 123,\n",
              " 'petrified': 124,\n",
              " 'amazing': 125,\n",
              " 'portion': 126,\n",
              " 'largely': 127,\n",
              " 'wall': 128,\n",
              " 'step': 129,\n",
              " 'down': 130,\n",
              " 'care': 131,\n",
              " 'friendly': 132,\n",
              " 'amount': 133,\n",
              " 'dos': 134,\n",
              " 'which': 135,\n",
              " 'ever': 136,\n",
              " 'recommendation': 137,\n",
              " '2': 138,\n",
              " 'behind': 139,\n",
              " 'towards': 140,\n",
              " 'mexican': 141,\n",
              " 'year': 142,\n",
              " 'familiar': 143,\n",
              " 'bottom': 144,\n",
              " 'chicken': 145,\n",
              " 'summary': 146,\n",
              " 'service': 147,\n",
              " 'horrible': 148,\n",
              " 'person': 149,\n",
              " 'value': 150,\n",
              " 'cash': 151,\n",
              " 'serves': 152,\n",
              " 'salad': 153,\n",
              " 'meh': 154,\n",
              " 'banana': 155,\n",
              " 'suck': 156,\n",
              " 'sever': 157,\n",
              " 'because': 158,\n",
              " 'itfriendly': 159,\n",
              " 'delicious': 160,\n",
              " 'damn': 161,\n",
              " 'minutes': 162,\n",
              " 'expected': 163,\n",
              " 'could': 164,\n",
              " 'me': 165,\n",
              " 'inside': 166,\n",
              " 'you': 167,\n",
              " 'human': 168,\n",
              " 'it': 169,\n",
              " 'batter': 170,\n",
              " 'fish': 171,\n",
              " 'beyond': 172,\n",
              " 'definitely': 173,\n",
              " 'try': 174,\n",
              " 'least': 175,\n",
              " 'quickly': 176,\n",
              " 'sunglasses': 177,\n",
              " 'excalibur': 178,\n",
              " 'sexy': 179,\n",
              " 'staff': 180,\n",
              " 'heard': 181,\n",
              " 'opportunity': 182,\n",
              " 'enjoy': 183,\n",
              " 'any': 184,\n",
              " 'too': 185,\n",
              " 'very': 186,\n",
              " 'drinks': 187,\n",
              " 'should': 188,\n",
              " 'stuff': 189,\n",
              " 'six': 190,\n",
              " 'of': 191,\n",
              " 'i': 192,\n",
              " 'shocked': 193,\n",
              " 'sucks': 194,\n",
              " 'up': 195,\n",
              " 'this': 196,\n",
              " 'duck': 197,\n",
              " 'use': 198,\n",
              " 'ripped': 199,\n",
              " 'other': 200,\n",
              " 'let': 201,\n",
              " 'appalling': 202,\n",
              " 'note': 203,\n",
              " 'poor': 204,\n",
              " 'way': 205,\n",
              " 'everything': 206,\n",
              " 'bank': 207,\n",
              " 'great': 208,\n",
              " 'by': 209,\n",
              " 'us': 210,\n",
              " 'before': 211,\n",
              " 'really': 212,\n",
              " 'positive': 213,\n",
              " 'so': 214,\n",
              " 'feel': 215,\n",
              " 'disgusted': 216,\n",
              " 'oh': 217,\n",
              " 'sweet': 218,\n",
              " 'section': 219,\n",
              " 'relationship': 220,\n",
              " 'raving': 221,\n",
              " 'deal': 222,\n",
              " 'did': 223,\n",
              " 'these': 224,\n",
              " 'ordered': 225,\n",
              " 'flatlined': 226,\n",
              " 'pucks': 227,\n",
              " 'tender': 228,\n",
              " 'like': 229,\n",
              " 'took': 230,\n",
              " 'attitudes': 231,\n",
              " 'a': 232,\n",
              " 'than': 233,\n",
              " 'bad': 234,\n",
              " 'ive': 235,\n",
              " 'today': 236,\n",
              " 'beer': 237,\n",
              " 'salt': 238,\n",
              " 'milk': 239,\n",
              " 'refreshing': 240,\n",
              " 'scallop': 241,\n",
              " 'eat': 242,\n",
              " 'trap': 243,\n",
              " 'nasty': 244,\n",
              " 'say': 245,\n",
              " 'for': 246,\n",
              " 'shrimp': 247,\n",
              " 'kept': 248,\n",
              " 'common': 249,\n",
              " 'side': 250,\n",
              " 'never': 251,\n",
              " 'tailored': 252,\n",
              " 'velvet': 253,\n",
              " 'hard': 254,\n",
              " 'potato': 255,\n",
              " 'redeeming': 256,\n",
              " 'over': 257,\n",
              " '10': 258,\n",
              " 'milkshake': 259,\n",
              " 'waitress': 260,\n",
              " 'prices': 261,\n",
              " 'nothing': 262,\n",
              " '30': 263,\n",
              " 'yourself': 264,\n",
              " 'overwhelmed': 265,\n",
              " 'nice': 266,\n",
              " 'would': 267,\n",
              " 'tasteless': 268,\n",
              " 'out': 269,\n",
              " 'forward': 270,\n",
              " 'roast': 271,\n",
              " 'tenders': 272,\n",
              " 'vegas': 273,\n",
              " 'updatewent': 274,\n",
              " 'where': 275,\n",
              " 'left': 276,\n",
              " 'sashimi': 277,\n",
              " 'rubber': 278,\n",
              " 'in': 279,\n",
              " 'blah': 280,\n",
              " 'bite': 281,\n",
              " 'fresh': 282,\n",
              " 'beautiful': 283,\n",
              " 'wow': 284,\n",
              " 'your': 285,\n",
              " 'appetizers': 286,\n",
              " 'best': 287,\n",
              " 'ample': 288,\n",
              " 'guess': 289,\n",
              " 'touch': 290,\n",
              " 'sick': 291,\n",
              " 'burrittos': 292,\n",
              " 'combos': 293,\n",
              " 'enough': 294,\n",
              " 'menu': 295,\n",
              " 'outside': 296,\n",
              " 'water': 297,\n",
              " 'has': 298,\n",
              " 'are': 299,\n",
              " 'on': 300,\n",
              " 'is': 301,\n",
              " 'may': 302,\n",
              " 'husband': 303,\n",
              " 'table': 304,\n",
              " 'do': 305,\n",
              " 'dining': 306,\n",
              " 'provided': 307,\n",
              " 'off': 308,\n",
              " 'spring': 309,\n",
              " 'place': 310,\n",
              " 'he': 311,\n",
              " 'chewy': 312,\n",
              " 'sense': 313,\n",
              " 'quality': 314,\n",
              " 'grease': 315,\n",
              " 'rare': 316,\n",
              " 'hole': 317,\n",
              " 'was': 318,\n",
              " 'register': 319,\n",
              " 'street': 320,\n",
              " 'look': 321,\n",
              " 'luke': 322,\n",
              " 'they': 323,\n",
              " 'receives': 324,\n",
              " 'stupid': 325,\n",
              " 'think': 326,\n",
              " 'absolutely': 327,\n",
              " 'smelled': 328,\n",
              " 'little': 329,\n",
              " 'handmade': 330,\n",
              " 'didnt': 331,\n",
              " 'good': 332,\n",
              " 'pho': 333,\n",
              " 'yummy': 334,\n",
              " 'moist': 335,\n",
              " 'perfectly': 336,\n",
              " 'potatoes': 337,\n",
              " 'each': 338,\n",
              " 'be': 339,\n",
              " 'downtown': 340,\n",
              " 'will': 341,\n",
              " 'party': 342,\n",
              " 'came': 343,\n",
              " 'talk': 344,\n",
              " 'now': 345,\n",
              " 'steve': 346,\n",
              " 'known': 347,\n",
              " 'more': 348,\n",
              " 'lot': 349,\n",
              " 'hummus': 350,\n",
              " 'walked': 351,\n",
              " 'omelets': 352,\n",
              " 'during': 353,\n",
              " 'company': 354,\n",
              " 'less': 355,\n",
              " 'salmon': 356,\n",
              " 'one': 357,\n",
              " 'underwhelming': 358,\n",
              " 'hottest': 359,\n",
              " 'quick': 360,\n",
              " 'wait': 361,\n",
              " 'with': 362,\n",
              " 'have': 363,\n",
              " 'worst': 364,\n",
              " 'burger': 365,\n",
              " 'styrofoam': 366,\n",
              " 'restaurant': 367,\n",
              " 'establishment': 368,\n",
              " 'running': 369,\n",
              " 'every': 370,\n",
              " 'experiencing': 371,\n",
              " 'experience': 372,\n",
              " 'refill': 373,\n",
              " 'warm': 374,\n",
              " 'what': 375,\n",
              " 'hair': 376,\n",
              " '4': 377,\n",
              " 'outrageously': 378,\n",
              " 'server': 379,\n",
              " 'pretty': 380,\n",
              " 'accident': 381,\n",
              " 'above': 382,\n",
              " 'that': 383,\n",
              " 'no': 384,\n",
              " 'bland': 385,\n",
              " 'again': 386,\n",
              " 'everyone': 387,\n",
              " 'attentive': 388,\n",
              " 'hiro': 389,\n",
              " 'dont': 390,\n",
              " 'been': 391,\n",
              " 'favor': 392,\n",
              " 'break': 393,\n",
              " 'beef': 394,\n",
              " 'struggle': 395,\n",
              " 'ratio': 396,\n",
              " 'tip': 397,\n",
              " 'after': 398,\n",
              " 'imaginative': 399,\n",
              " 'disgust': 400,\n",
              " 'times': 401,\n",
              " 'tacos': 402,\n",
              " 'warmer': 403,\n",
              " 'customer': 404,\n",
              " 'into': 405,\n",
              " 'seems': 406,\n",
              " 'cocktails': 407,\n",
              " 'ask': 408,\n",
              " 'wasting': 409,\n",
              " 'sandwich': 410,\n",
              " 'when': 411,\n",
              " 'highly': 412,\n",
              " 'worth': 413,\n",
              " 'taste': 414,\n",
              " 'old': 415,\n",
              " 'tasty': 416,\n",
              " 'firehouse': 417,\n",
              " 'some': 418,\n",
              " 'mouth': 419,\n",
              " 'texture': 420,\n",
              " 'late': 421,\n",
              " 'pub': 422,\n",
              " 'rolls': 423,\n",
              " 'pink': 424,\n",
              " '5': 425,\n",
              " 'sugary': 426,\n",
              " 'prompt': 427,\n",
              " 'were': 428,\n",
              " 'excuse': 429,\n",
              " 'cant': 430,\n",
              " 'am': 431,\n",
              " 'thats': 432,\n",
              " 'glad': 433,\n",
              " 'wave': 434,\n",
              " 'decent': 435,\n",
              " 'realized': 436,\n",
              " 'inexpensive': 437,\n",
              " 'honeslty': 438,\n",
              " 'unsatisfying': 439,\n",
              " 'disappointing': 440,\n",
              " 'recommended': 441,\n",
              " 'always': 442,\n",
              " 'stars': 443,\n",
              " 'cakeohhh': 444,\n",
              " 'whether': 445,\n",
              " 'made': 446,\n",
              " 'both': 447,\n",
              " 'wonderful': 448,\n",
              " 'here': 449,\n",
              " 'rock': 450,\n",
              " 'fries': 451,\n",
              " 'being': 452,\n",
              " 'strings': 453,\n",
              " 'cape': 454,\n",
              " 'attack': 455,\n",
              " 'angry': 456,\n",
              " 'others': 457,\n",
              " 'love': 458,\n",
              " 'blow': 459,\n",
              " 'ahead': 460,\n",
              " 'ravoli': 461,\n",
              " 'around': 462,\n",
              " 'seasoned': 463,\n",
              " 'char': 464,\n",
              " 'servers': 465,\n",
              " 'military': 466,\n",
              " 'quite': 467,\n",
              " 'ended': 468,\n",
              " 'its': 469,\n",
              " 'stopped': 470,\n",
              " 'heart': 471,\n",
              " 'casino': 472,\n",
              " 'eating': 473,\n",
              " 'but': 474,\n",
              " 'discount': 475,\n",
              " 'holiday': 476,\n",
              " 'turkey': 477,\n",
              " 'min': 478,\n",
              " 'going': 479,\n",
              " 'breakfast': 480,\n",
              " 'die': 481,\n",
              " 'slow': 482}"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word2Vec.word_to_index_dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_6kWcdS4QZzM"
      },
      "source": [
        "Now define your CBOW model class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "V-sMXIE6heYv"
      },
      "outputs": [],
      "source": [
        "# Define your CBOW model here (TODO)\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M1gYhtWBx88h"
      },
      "source": [
        "## Train the CBOW model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LuWSX_ZGnPIG"
      },
      "source": [
        "Now that your model class is written, you must create an instance of the model and train it using the loss function `torch.nn.CrossEntropyLoss` on the output of `fc2` and `y` (the labels).\n",
        "\n",
        "Train your CBOW model for 300 epochs with `embed_dim`= 100, `window_size`=2, and `hidden_dim`=30. \n",
        "- Do not split the data into training and test sets (we will not be evaluating the performance of this model). \n",
        "- Use the SGD optimizer with learning rate = 0.001.\n",
        "- Append the loss at every epoch to a list (return the list if you use a function to fit your model), so that we can plot it later. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "35ybUlZs14Ud"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "VOCAB_SIZE = len(vocab)\n",
        "EMBED_DIM = 100\n",
        "WINDOW_SIZE = 2\n",
        "HIDDEN_DIM = 30\n",
        "N_EPOCHS = 300\n",
        "\n",
        "# Construct input data\n",
        "data = word2Vec.createTupleList(cleaned_text, WINDOW_SIZE, word2Vec.word_to_index_dictionary)\n",
        "\n",
        "# Train your CBOW model here (TODO)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(tensor([115, 416, 109, 420]), tensor([36])),\n",
              " (tensor([416,  36, 420, 318]), tensor([109])),\n",
              " (tensor([ 36, 109, 318,   5]), tensor([420])),\n",
              " (tensor([109, 420,   5, 244]), tensor([318])),\n",
              " (tensor([470, 209, 109, 421]), tensor([353]))]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lE_aC7oJx2xC"
      },
      "source": [
        "## Plot losses by epochs (x-axis: epoch, y-axis: loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "xdAEHlvX14Ur"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "In C:\\Users\\wangy\\Anaconda3\\envs\\SUSHI\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
            "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
            "In C:\\Users\\wangy\\Anaconda3\\envs\\SUSHI\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
            "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
            "In C:\\Users\\wangy\\Anaconda3\\envs\\SUSHI\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
            "In C:\\Users\\wangy\\Anaconda3\\envs\\SUSHI\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
            "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
            "In C:\\Users\\wangy\\Anaconda3\\envs\\SUSHI\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
            "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
            "In C:\\Users\\wangy\\Anaconda3\\envs\\SUSHI\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
            "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
            "In C:\\Users\\wangy\\Anaconda3\\envs\\SUSHI\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
            "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
            "In C:\\Users\\wangy\\Anaconda3\\envs\\SUSHI\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
            "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
          ]
        }
      ],
      "source": [
        "# Insert your code here to plot losses vs epoch (TODO)\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GWj-u7YJFxF7"
      },
      "source": [
        "## Print five most similar words with the word \"delicious\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vvxTFjD7GCDc"
      },
      "source": [
        "The whole point of traiing an embedding model is to get an embedding vector for each word. The idea is that the vector somehow captures the meaning of the word. This is useful because data scientists often face scenarios where they must derive meaning from unstructured text data.\n",
        "\n",
        "Once your model has been trained, you can access the embedding vectors through `model.embedding.weight.data`. You can convert these vectors to a numpy matrix or numpy arrays if needed.\n",
        "\n",
        "Find the five most similar words with the word \"delicious\" by calculating cosine similarity between the embedding vector of \"delicious\" and the embedding vectors of all other words in the vocabulary. \n",
        "\n",
        "Hint: cosine similarity is a common metric so you should be able to find one that you can use in an existing library. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "rS5bfhBW14U_"
      },
      "outputs": [],
      "source": [
        "# Insert your code here (TODO)\n",
        "import sklearn.metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UJrmuZEE5ETn"
      },
      "source": [
        "While the model learns embedding vectors (that best predict the focal word from its contexts), the vectors that it learns don't seem to truly capture the meaning of words. However, this is mainly due to the small size of our training data. Google trained a word2vec model based on large-scale data (about 100 billion words), and this model captures similarity between words well. You can find the pretrained model at https://code.google.com/archive/p/word2vec/."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "BA865 - HW03.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "3aad574622dbe04c5ad2ac963758ca54718a680159ab2b2e32c356155ad01d41"
    },
    "kernelspec": {
      "display_name": "Python 3.7.9 64-bit ('SUSHI': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
